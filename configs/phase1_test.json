{
    "description": "Phase 1 - Test Configuration (Smaller Scale)",
    "phase": 1,
    "target_tokens": 50000000,
    
    "mix": {
        "description": "Token mixing by percentage",
        "clean_web": 0.60,
        "wikipedia": 0.40
    },
    
    "data_sources": [
        {
            "name": "clean_web",
            "dataset": "allenai/c4",
            "config_name": "en",
            "text_field": "text",
            "weight": 0.60,
            "description": "Clean web text - C4 dataset"
        },
        {
            "name": "wikipedia",
            "dataset": "wikimedia/wikipedia",
            "config_name": "20231101.en",
            "text_field": "text",
            "weight": 0.40,
            "description": "English Wikipedia"
        }
    ],
    
    "training": {
        "learning_rate": 3e-4,
        "batch_size": 16,
        "gradient_accumulation_steps": 4,
        "weight_decay": 0.1,
        "log_interval": 20,
        "eval_interval": 500,
        "save_interval": 500,
        "context_ramping": {
            "enabled": true,
            "stages": [
                {
                    "context_length": 1024,
                    "at_tokens": 0.0
                },
                {
                    "context_length": 2048,
                    "at_tokens": 0.6
                }
            ]
        },
        "temperature_schedule": {
            "initial": 0.8,
            "final": 0.7,
            "ramp_end_tokens": 0.5
        }
    },
    
    "model": {
        "hidden_size": 768,
        "num_layers": 12,
        "num_heads": 12,
        "max_seq_len": 4096,
        "enable_router": true,
        "enable_schema_inference": true,
        "enable_hybrid_memory": true,
        "enable_self_correction": true
    },
    
    "memory": {
        "entity_capacity": 100000,
        "passage_capacity": 50000,
        "fact_extraction_interval": 4,
        "ingestion_batch_size": 4
    }
}